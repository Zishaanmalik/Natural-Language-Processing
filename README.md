# Natural Language Processing

This repository is a hands-on collection of Natural Language Processing (NLP) projects and experiments.  
All work here is built using Python and Jupyter Notebooks, so anyone can open, run, and learn step by step.

The notebooks explore many core NLP techniques from cleaning text to building machine-learning models  and include real examples like Word2Vec, POS tagging, text classification etc.

---

## ğŸ“‚ Whatâ€™s Inside

Youâ€™ll find the following key parts:

### ğŸ§° Core NLP Notebooks
- **Text Preprocessing.ipynb** â€” How to clean and prepare raw text  
- **Vectorization_Techniques.ipynb** â€” Turning text into numbers  
- **gensim.ipynb** â€” Using gensim for Word2Vec and related embedding work  
- **pos-tagging.ipynb** â€” Part-of-speech tagging basics

### ğŸ§  NLP Projects
- **text-classification using machine learning.ipynb**  
  A classic ML approach to classify text

- **text-classification with word2vec using machine learning.ipynb**  
  Uses Word2Vec embeddings with ML models for better performance

- **word2vec game_of_thrones.ipynb**  
  Word2Vec visualizations and experiments with fantasy text

- **word2vec of story.ipynb**  
  Another Word2Vec walkthrough on custom story text

### âœ‰ï¸ Additional Script
- **preceptron Email spam.py**  
  A small Python script showing a Perceptron model for email spam detection

### ğŸ“ Data and Models
- **data/** â€” Datasets used in notebooks  
- **models/** â€” Saved vector models, embeddings, and any trained artifacts

### âœ¨ Other Files
- `.idea/` and `.ipynb_checkpoints/` â€” IDE and notebook metadata

---

## ğŸ§  What This Repository Shows

This is not a single project â€” itâ€™s a **learning collection** that walks through many core NLP concepts:

### ğŸ“Œ Text Cleaning & Preprocessing

Before any model, text needs cleaning:
- Lowercasing
- Removing punctuation
- Tokenization
- Stopwords removal

These steps make raw sentences ready for numeric processing.

---

### ğŸ“Œ Vectorization Techniques

Words and text canâ€™t be fed to models directly.  
This repo shows:
- Bag of Words
- TF-IDF
- Word2Vec embeddings

You learn how text becomes numeric vectors.

---

### ğŸ“Œ ML vs Word2Vec + ML

You will see how classic machine-learning classifiers behave on raw text vs when paired with powerful Word2Vec embeddings.  
Word2Vec often creates meaningful numerical representations that help ML models perform better.

---

### ğŸ“Œ POS Tagging

Understanding grammar can help with certain downstream tasks â€” so thereâ€™s an example of part-of-speech tagging too.

---

### ğŸ“Œ Word2Vec Explorations

The notebooks include Word2Vec experiments on fun text like:
- Game of Thrones
- Story text

These help visualize word relationships and show how embedding spaces capture semantic similarity.

---

## ğŸš€ How to Run

1. Clone the repo:
   ```bash
   git clone https://github.com/Zishaanmalik/Natural-Language-Processing.git
